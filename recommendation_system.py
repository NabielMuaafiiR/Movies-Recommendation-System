# -*- coding: utf-8 -*-
"""Recommendation System

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RfH0t7yLMaX4NuOWSPEBCbZ0K_0Jc_6H

# **Import Package**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
drive.mount('/content/drive')
import re

from sklearn.model_selection import train_test_split

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import tensorflow as tf
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping

"""import library yang di perlukan

# **Data Preparation**
"""

df_movie = pd.read_csv("/content/drive/MyDrive/DBS/ML TERAPAN/ML Terapan 2/Data/movies.csv")
df_rating = pd.read_csv("/content/drive/MyDrive/DBS/ML TERAPAN/ML Terapan 2/Data/ratings.csv")

"""Membaca data, df_movie untuk data daftar film dengan movieId sebagai index dan df_rating untuk data user memberi rating dengan userId sebagai index"""

df_movie.head()

"""Membaca 5 data teratas dari data daftar film"""

df_rating.head()

"""Membaca 5 data teratas dari rating user

# **Pre-Processing Data**

## Data Undertand
"""

movieShape = df_movie.shape
ratingShape = df_rating.shape

print(f'Ukuran dari data movie baris: {movieShape[0]} dan Kolom: {movieShape[1]}')
print(f'Ukuran dari data rating baris: {ratingShape[0]} dan Kolom: {ratingShape[1]}')

df_movie.info()

"""Terdapat 2 variabel pada data movie, keduanya merupakan kategorik"""

df_rating.info()

"""Terdapat 3 variabel pada data rating, dengan ketiganya merupakan data numerik"""

df_movie.isna().sum()

"""Tidak terdapat data kosong pada data movie"""

df_rating.isna().sum()

"""Tidak terdapat data kosong pada data rating"""

df_movie.duplicated().sum()

"""Tidak terdapat duplikasi data pada data movie"""

df_rating.duplicated().sum()

"""Tidak terdapat data duplikasi pada data rating"""

print('Jumlah userID: ', len(df_rating.userId.unique()))
print('Jumlah movieId: ', len(df_rating.movieId.unique()))
print('Jumlah data rating: ', len(df_rating))

"""Terdapat 610 user yang melakukan penilaian, dari 9.724 film dengan total dari data rating 100.836

## EDA
"""

df_rating.describe()

"""Deskripsi statistika terkait data rating"""

genres = df_movie['genres'].str.get_dummies('|').sum().sort_values(ascending=False)

# Plot the bar chart
plt.figure(figsize=(15, 8))
sns.barplot(x=genres.index, y=genres.values)
plt.title('Total Number of Movies per Genre')
plt.xlabel('Genre')
plt.ylabel('Number of Movies')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

"""Melihat sebaran genre film

## Data Preparation
"""

def clean(text):
  text = text.lower()
  text = re.sub(r'\|', ' ', text)
  return text

"""Mempersiapkan pembersihan data"""

df_movie_clean = df_movie.copy()
df_movie_clean['genres'] = df_movie_clean['genres'].apply(clean)
df_movie_clean.head()

"""Data genre dibersihkan"""

vectorizer = TfidfVectorizer()
tfidf = vectorizer.fit_transform(df_movie_clean['genres'])

"""Melakukan vektorisasi dengan TF-IDF"""

min_rating = df_rating['rating'].min()
max_rating = df_rating['rating'].max()

"""Menghitung max dan min nilai rating"""

unique_movie_ids = df_rating['movieId'].unique()
movie_to_index = {movie_id: index for index, movie_id in enumerate(unique_movie_ids)}

unique_user_ids = df_rating['userId'].unique()
user_to_index = {user_id: index for index, user_id in enumerate(unique_user_ids)}

# Apply the mapping to the original dataframe before splitting
df_rating['userId_mapped'] = df_rating['userId'].map(user_to_index)
df_rating['movieId_mapped'] = df_rating['movieId'].map(movie_to_index)

"""Mempersiapkan data sebelum masuk kedalam model"""

x = df_rating[['userId_mapped', 'movieId_mapped']] # Use mapped IDs
y = df_rating['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

"""Mempersiapkan data x dan y, lalu dilakukan splitting

# Training
"""

similarity = cosine_similarity(tfidf)
print(similarity)

"""Melakukan perhitungan similaritas pada tabel movie"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.movie_embedding = layers.Embedding( # layer embeddings movie
        num_movie,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_movie, 1) # layer embedding movie bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    movie_vector = self.movie_embedding(inputs[:, 1]) # memanggil layer embedding 3
    movie_bias = self.movie_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)

    x = dot_user_movie + user_bias + movie_bias

    return tf.nn.sigmoid(x) # activation sigmoid

early_stopping_cb = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

"""Membangun model rekomendasi"""

num_users = len(df_rating.userId.unique())
movie_len = len(df_movie)
model = RecommenderNet(num_users, movie_len, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Melakukan compiling model"""

history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = 16,
    epochs = 50,
    callbacks = [early_stopping_cb],
    validation_data = (X_test, y_test)
)

"""Melakukan training

# Evaluasi
"""

def precision_at_k(similarity, df_movie_clean, k=5, num_eval_movies=100):
    num_movies = similarity.shape[0]
    random_movie_indices = np.random.choice(num_movies, num_eval_movies, replace=False)
    total_precision = 0

    genre_sets = [
        set(genres.split()) if pd.notnull(genres) else set()
        for genres in df_movie_clean['genres']
    ]

    for target_idx in random_movie_indices:
        similarity_scores = similarity[target_idx]
        sorted_indices = np.argsort(similarity_scores)[::-1]
        # Exclude the target movie itself
        recommended_indices = [i for i in sorted_indices if i != target_idx][:k]

        if not recommended_indices:
            continue  # Skip if no recommendations are made

        target_genres = genre_sets[target_idx]

        # Define "relevant" recommendations based on shared genres
        relevant_recommended_count = 0
        for rec_idx in recommended_indices:
            if len(target_genres.intersection(genre_sets[rec_idx])) > 0:
                relevant_recommended_count += 1

        total_precision += relevant_recommended_count / k

    return total_precision / num_eval_movies if num_eval_movies > 0 else 0

precision_value = precision_at_k(similarity, df_movie_clean, k=5, num_eval_movies=100)
print(f"Precision@5: {precision_value}")

"""Evaluasi model content based filtering dengan recall@5"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Melihat grafik training dan testing

# Inference
"""

def find_similar_movies(movie_id, num_similar=10):
    # Find the index of the given movie_id
    try:
        movie_index = df_movie_clean[df_movie_clean['movieId'] == movie_id].index[0]
    except IndexError:
        print(f"Movie ID {movie_id} not found in the dataset.")
        return pd.DataFrame()

    # Get the similarity scores for this movie with all other movies
    similarity_scores = list(enumerate(similarity[movie_index]))

    # Sort the movies based on similarity scores in descending order
    sorted_similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)

    # Get the indices of the most similar movies (exclude the movie itself)
    top_movie_indices = [i[0] for i in sorted_similarity_scores[1:num_similar+1]]

    # Get the movie details for the similar movies
    similar_movies = df_movie_clean.iloc[top_movie_indices][['title', 'genres']]

    return similar_movies

# Find similar movies for a movie with movieId 50 (Usual Suspects, The)
movie_id_to_find_similar = 50
similar_movies = find_similar_movies(movie_id_to_find_similar, num_similar=10)

if not similar_movies.empty:
    print(f"\nTop 10 similar movies for Movie ID {movie_id_to_find_similar} ({df_movie_clean[df_movie_clean['movieId'] == movie_id_to_find_similar]['title'].values[0]}):")
similar_movies

"""Melakukan inference pada content based"""

def recommend_for_user(user_id, num_recommendations=10):
  user_index = user_to_index.get(user_id)
  if user_index is None:
    print(f"User ID {user_id} not found in the training data.")
    return pd.DataFrame()

  movies_not_rated_by_user = df_movie_clean[~df_movie_clean['movieId'].isin(df_rating[df_rating['userId'] == user_id]['movieId'])]['movieId'].values
  movies_not_rated_mapped = [movie_to_index[movie_id] for movie_id in movies_not_rated_by_user if movie_id in movie_to_index]

  if not movies_not_rated_mapped:
      print(f"User ID {user_id} has rated all movies or no new movies are available for recommendation.")
      return pd.DataFrame()

  user_tensor = tf.constant([user_index] * len(movies_not_rated_mapped), dtype=tf.int32)
  movies_tensor = tf.constant(movies_not_rated_mapped, dtype=tf.int32)

  # Stack the user and movie tensors into a single input tensor
  model_input = tf.stack([user_tensor, movies_tensor], axis=1)

  # Make predictions
  predicted_ratings = model.predict(model_input)

  # Get the top recommended movie indices
  top_movie_indices = np.argsort(predicted_ratings.flatten())[::-1][:num_recommendations]

  # Map the recommended movie indices back to original movie IDs
  index_to_movie = {index: movie_id for movie_id, index in movie_to_index.items()}
  recommended_movie_ids = [index_to_movie[movies_not_rated_mapped[i]] for i in top_movie_indices]

  # Get the movie details for the recommended movies
  recommended_movies = df_movie_clean[df_movie_clean['movieId'].isin(recommended_movie_ids)]

  return recommended_movies[['title', 'genres']]

# Example usage:
user_id_to_recommend_for = 1 # Replace with the desired user ID
recommendations = recommend_for_user(user_id_to_recommend_for, num_recommendations=10)

if not recommendations.empty:
    print(f"Top 10 movie recommendations for User ID {user_id_to_recommend_for}:")
recommendations

"""Melakukan inference pada user-based"""